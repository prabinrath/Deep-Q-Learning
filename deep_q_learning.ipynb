{"cells":[{"cell_type":"code","execution_count":1,"id":"U8g5UFDxNY6o","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5838,"status":"ok","timestamp":1671669530721,"user":{"displayName":"Prabin Kumar Rath","userId":"14553073823098569113"},"user_tz":420},"id":"U8g5UFDxNY6o","outputId":"820d16d6-383e-4c78-8b35-44a7aa3a4a38"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[atari] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (1.21.6)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (5.1.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (1.5.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (0.0.8)\n","Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (0.7.5)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.10.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.11.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license]) (1.5.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license]) (1.21.6)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license]) (5.1.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license]) (0.0.8)\n","Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license]) (0.4.2)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (5.10.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.23.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (7.1.2)\n","Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license]) (3.11.0)\n","Requirement already satisfied: libtorrent in /usr/local/lib/python3.8/dist-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2022.12.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.24.3)\n"]}],"source":["!pip install gym[atari]\n","!pip install gym[accept-rom-license]"]},{"cell_type":"code","execution_count":2,"id":"431d2e9d-9132-4f3d-93af-1617e020b061","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2300,"status":"ok","timestamp":1671669533014,"user":{"displayName":"Prabin Kumar Rath","userId":"14553073823098569113"},"user_tz":420},"id":"431d2e9d-9132-4f3d-93af-1617e020b061","outputId":"f0a152df-74dc-457b-f4a8-91a61c1c5df1"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import gym\n","import numpy as np\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from copy import deepcopy\n","import random\n","import cv2\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":4,"id":"5caa7c10-f7a9-4ee1-8acf-18779ff39e94","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1671669534872,"user":{"displayName":"Prabin Kumar Rath","userId":"14553073823098569113"},"user_tz":420},"id":"5caa7c10-f7a9-4ee1-8acf-18779ff39e94"},"outputs":[],"source":["class AtariEnv():\n","    def __init__(self, name):\n","        self.env = gym.make(name, render_mode=\"rgb_array\")\n","        self.obs_dim = self.env.observation_space.shape[0]\n","        self.act_dim = self.env.action_space.n\n","        self.action_space = self.env.action_space\n","        self.n_buffer = 4\n","        self.buffer = None\n","\n","    def pre_process(self, observation):\n","        '''\n","        State Preprocessing\n","        '''\n","        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)  \n","        reshaped = cv2.resize(gray, (84,110))\n","        cropped = reshaped[18:102,:]/255\n","        return np.expand_dims(cropped, 0)\n","\n","    def get_state(self):\n","        if self.buffer == None:\n","            self.reset()\n","        return np.expand_dims(np.vstack(self.buffer), 0)\n","\n","    def get_reward(self, observation, reward):\n","        return reward\n","\n","    def reset(self, seed=None):\n","        if seed:\n","            observation = self.env.reset(seed)\n","        else:\n","            observation = self.env.reset()\n","        observation = self.pre_process(observation)\n","        self.buffer = [observation,]*self.n_buffer\n","\n","    def step(self, action):\n","        observation, reward, terminated, _ = self.env.step(action)\n","        observation = self.pre_process(observation)\n","        self.buffer.pop(0)\n","        self.buffer.append(observation)\n","        next_state = self.get_state()\n","        if terminated:\n","            self.buffer = None\n","        return next_state, self.get_reward(observation, reward), terminated\n","    \n","    def render(self):\n","        return self.env.render()/255"]},{"cell_type":"code","execution_count":5,"id":"6fdfe701-d6a9-444d-8b29-94a5c23d641b","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1671669534873,"user":{"displayName":"Prabin Kumar Rath","userId":"14553073823098569113"},"user_tz":420},"id":"6fdfe701-d6a9-444d-8b29-94a5c23d641b"},"outputs":[],"source":["class AtariLearner(nn.Module):\n","    def __init__(self, in_channels, act_dim):\n","        super().__init__()\n","        self.cnn1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=8, stride=4)\n","        h, w = self.calc_conv2d_output_dim(in_dim = (84,84), kernel_size=(8,8), stride=(4,4))\n","        self.bn1 = nn.BatchNorm2d(16)\n","        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n","        h, w = self.calc_conv2d_output_dim(in_dim = (h,w), kernel_size=(4,4), stride=(2,2))\n","        self.bn2 = nn.BatchNorm2d(32)\n","        self.fcc1 = nn.Linear(h*w*32, 256)\n","        self.fcc2 = nn.Linear(256, act_dim)\n","\n","    def calc_conv2d_output_dim(self, in_dim, kernel_size, padding=(0,0), dialation=(1,1), stride=(1,1)):\n","        h = (in_dim[0]+2*padding[0]-dialation[0]*(kernel_size[0]-1)-1)//stride[0]+1\n","        w = (in_dim[1]+2*padding[1]-dialation[1]*(kernel_size[1]-1)-1)//stride[1]+1\n","        return h, w\n","\n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.cnn1(x)))\n","        x = F.relu(self.bn2(self.cnn2(x)))\n","        x = x.view(x.size(0),-1)\n","        x = F.relu(self.fcc1(x))\n","        return self.fcc2(x)"]},{"cell_type":"code","execution_count":6,"id":"c4dfb027-02a9-49b5-9b14-8b50bf476ca8","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671669534874,"user":{"displayName":"Prabin Kumar Rath","userId":"14553073823098569113"},"user_tz":420},"id":"c4dfb027-02a9-49b5-9b14-8b50bf476ca8"},"outputs":[],"source":["class BatchReplayMemory():\n","    def __init__(self, n_buffer, max_len=10000):\n","        self.queue = []\n","        self.max_len = max_len\n","        self.n_buffer = n_buffer\n","    \n","    def push(self, data):\n","        self.queue.append(data)\n","        if len(self.queue)>self.max_len:\n","            self.queue.pop(0)\n","    \n","    def sample(self, batch_size):\n","        batch = []\n","        while len(batch)<batch_size:\n","            idx = random.sample(range(len(self.queue)),1)[0]\n","            if idx >= self.n_buffer:\n","                s, a, r, n, t = zip(*self.queue[idx-self.n_buffer:idx])\n","                interm_t = False\n","                for i in t[:-1]:\n","                    interm_t = interm_t or i\n","                    \n","                # Abrupt transitions should not be sampled\n","                if not interm_t:\n","                    batch.append((np.expand_dims(np.vstack(s),0),a[-1],r[-1],np.expand_dims(np.vstack(n),0),t[-1]))\n","        return batch\n","    \n","    def length(self):\n","        return len(self.queue)"]},{"cell_type":"code","execution_count":7,"id":"8355d6e6-98e9-45d4-987d-a01c58c36af0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2271,"status":"ok","timestamp":1671669537140,"user":{"displayName":"Prabin Kumar Rath","userId":"14553073823098569113"},"user_tz":420},"id":"8355d6e6-98e9-45d4-987d-a01c58c36af0","outputId":"6c8f3ad9-92e1-413e-ffc2-128066434866"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["# Constant Parameters\n","RENDER = False\n","GAMMA = 0.98 # Discount factor\n","UPDATE_INTERVAL = 1000 # Interval for target update\n","LR = 0.001 # AdamW learning rate\n","EPSILON_START = 0.9 # Annealing start\n","EPSILON_END = 0.05 # Annealing end\n","EXPLORATION_FRAMES = 1000000 # Annealing frames\n","BATCH_SIZE = 64 # Sampling size from memory\n","MEMORY_BUFFER = 50000 # Replay buffer size\n","EPISODES = 1000 # Number of episodes for training\n","\n","environment = 'PongDeterministic-v4'\n","# environment, training policy, target policy\n","env = AtariEnv(environment)\n","policy = AtariLearner(env.n_buffer, env.act_dim).double().to(device)\n","target = AtariLearner(env.n_buffer, env.act_dim).double().to(device)\n","target.load_state_dict(policy.state_dict())\n","renv = deepcopy(env)\n","\n","loss_fn = nn.SmoothL1Loss()\n","optimizer = optim.AdamW(policy.parameters(), lr=LR, amsgrad=True)\n","\n","# Memory for Experience Replay\n","memory = BatchReplayMemory(env.n_buffer, MEMORY_BUFFER)\n","glob_frame = 0"]},{"cell_type":"code","execution_count":8,"id":"b4899b65-49f7-465e-9b7d-01f8177dab7f","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671669537140,"user":{"displayName":"Prabin Kumar Rath","userId":"14553073823098569113"},"user_tz":420},"id":"b4899b65-49f7-465e-9b7d-01f8177dab7f"},"outputs":[],"source":["def get_epsilon():\n","    # Linear Annealing\n","    return EPSILON_END + (EXPLORATION_FRAMES-glob_frame)*(EPSILON_START-EPSILON_END)/EXPLORATION_FRAMES \\\n","        if glob_frame < EXPLORATION_FRAMES else EPSILON_END\n","\n","def select_action(state, act_dim, eps=None):    \n","    if eps==None:\n","        eps = get_epsilon()\n","    # Epsilon-greedy exploration\n","    if np.random.uniform() < eps:\n","        return np.random.choice(act_dim)\n","    else:\n","        with torch.no_grad():\n","            policy.eval()\n","            q_sa = policy(torch.tensor(state, device=device))\n","        return torch.argmax(q_sa[0]).item()\n","\n","def optimize_policy(samples):\n","    states, actions, rewards, next_states, terminals = zip(*samples)\n","    states = torch.tensor(np.vstack(states), device=device)\n","    actions = torch.tensor(np.vstack(actions), device=device)\n","    next_states = torch.tensor(np.vstack(next_states), device=device)\n","    policy.train()\n","    q_sa = policy(states).gather(1, actions).squeeze()\n","    with torch.no_grad():\n","        target.eval()\n","        q_nsa_max = target(next_states).max(1).values\n","    q_sa_target = [rewards[j]+GAMMA*q_nsa_max[j].item()*(1-terminals[j]) for j in range(len(rewards))]\n","    q_sa_target = torch.tensor(q_sa_target, device=device)\n","    # Optimize on the TD loss\n","    loss = loss_fn(q_sa, q_sa_target)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()            \n","\n","def validate_policy():    \n","    renv.reset()\n","    done = False\n","    valid_reward = 0\n","    if RENDER:\n","        cv2.namedWindow(environment, cv2.WINDOW_NORMAL)\n","    # cv2.resizeWindow(environment, 300, 300)\n","    while not done:       \n","        state = renv.get_state()\n","        if RENDER:\n","            rgb = renv.render()\n","            cv2.imshow(environment, rgb)\n","            cv2.waitKey(10)\n","        action = select_action(state, renv.act_dim, EPSILON_END)\n","        _, reward, done = renv.step(action)\n","        valid_reward+=reward\n","    return valid_reward"]},{"cell_type":"code","execution_count":null,"id":"b94e706b-b100-41f0-80aa-aa450bc315d7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b94e706b-b100-41f0-80aa-aa450bc315d7","outputId":"17defb2d-60a1-4072-9b6e-80230a6b8154"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n"]},{"name":"stdout","output_type":"stream","text":["Episode:  0  | Validation Reward:  -20.0  | Epsilon:  0.9\n","Episode:  1  | Validation Reward:  -21.0  | Epsilon:  0.8989545\n","Episode:  2  | Validation Reward:  -21.0  | Epsilon:  0.89792345\n","Episode:  3  | Validation Reward:  -20.0  | Epsilon:  0.89696635\n","Episode:  4  | Validation Reward:  -21.0  | Epsilon:  0.8961002\n","Episode:  5  | Validation Reward:  -21.0  | Epsilon:  0.8951737\n","Episode:  6  | Validation Reward:  -21.0  | Epsilon:  0.8940142999999999\n","Episode:  7  | Validation Reward:  -21.0  | Epsilon:  0.89303595\n","Episode:  8  | Validation Reward:  -21.0  | Epsilon:  0.8920431500000001\n","Episode:  9  | Validation Reward:  -21.0  | Epsilon:  0.8909254000000001\n","Episode:  10  | Validation Reward:  -21.0  | Epsilon:  0.89005755\n","Episode:  11  | Validation Reward:  -20.0  | Epsilon:  0.8888854\n","Episode:  12  | Validation Reward:  -21.0  | Epsilon:  0.88793425\n","Episode:  13  | Validation Reward:  -21.0  | Epsilon:  0.88703835\n","Episode:  14  | Validation Reward:  -21.0  | Epsilon:  0.8857336\n","Episode:  15  | Validation Reward:  -21.0  | Epsilon:  0.88485045\n","Episode:  16  | Validation Reward:  -20.0  | Epsilon:  0.88385\n"]}],"source":["max_possible_reward = 21\n","reward_increment = max_possible_reward/10\n","max_valid_reward = -21\n","reward_history = []\n","max_reward_target = max_valid_reward + reward_increment\n","for episode in range(EPISODES):\n","    # if max_valid_reward > max_possible_reward*0.98:\n","    #     RENDER = True\n","    valid_reward = validate_policy()\n","    print('Episode: ', episode, ' | Validation Reward: ', valid_reward, ' | Epsilon: ', get_epsilon())\n","    max_valid_reward = max(valid_reward,max_valid_reward)\n","    reward_history.append(valid_reward)\n","\n","    # Save model when there is a performance improvement\n","    if max_valid_reward>max_reward_target:\n","        max_reward_target = min(max_possible_reward, max(max_reward_target,max_valid_reward)+reward_increment)-1    \n","        print('Performance Improvement!')    \n","        print('Episode: ', episode, ' | Max Validation Reward: ', max_valid_reward, ' | Epsilon: ', get_epsilon())\n","        torch.save(policy.state_dict(), path+'/'+environment+'/'+str(int(max_valid_reward))+'.dqn')\n","        if max_valid_reward==max_possible_reward:\n","            print('Best Model Achieved !!!')\n","            break\n","    \n","    # Default max episode steps is defined in Gym environments\n","    done = False\n","    while not done:       \n","        state = env.get_state()\n","        action = select_action(state, env.act_dim)\n","        next_state, reward, done = env.step(action)        \n","        glob_frame+=1\n","\n","        memory.push((state[:,env.n_buffer-1,:,:], action, reward, next_state[:,env.n_buffer-1,:,:], done))\n","        if memory.length()<MEMORY_BUFFER*0.5:\n","            continue\n","        else:\n","            optimize_policy(memory.sample(BATCH_SIZE))\n","\n","        if glob_frame%UPDATE_INTERVAL==0:\n","            target.load_state_dict(policy.state_dict())"]},{"cell_type":"code","execution_count":null,"id":"dabe693a-6435-4101-a4f6-0eb8139fe5c8","metadata":{"id":"dabe693a-6435-4101-a4f6-0eb8139fe5c8"},"outputs":[],"source":["# RENDER = True\n","# validate_policy()\n","\n","reward_history = np.array(reward_history)\n","smooth_reward_history = np.convolve(reward_history, np.ones(20)/20, mode='same')\n","import matplotlib.pyplot as plt\n","plt.plot(reward_history, label='Real')\n","plt.plot(smooth_reward_history, label='Smooth')\n","plt.xlabel('Episode')\n","plt.ylabel('Reward')\n","plt.legend(loc='upper left')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.10 ('torch-env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"bb2dc019e7f62d23a3168196ee78b6f9b4a4d9cea5fc9d3da8f6f7e2cbc333fd"}}},"nbformat":4,"nbformat_minor":5}
